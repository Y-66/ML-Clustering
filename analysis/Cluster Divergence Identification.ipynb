{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90934321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# ===================== 1. Load Data =====================\n",
    "\n",
    "df = pd.read_csv('../processed_data.csv')\n",
    "docs = df['Cleaned_Content'].fillna('').str.replace(r'\\bpad\\b', '', regex=True).tolist()\n",
    "true_labels = df['Label'].tolist()\n",
    "\n",
    "unique_labels = sorted(list(set(true_labels)))\n",
    "label_to_id = {l: i for i, l in enumerate(unique_labels)}\n",
    "y_true = np.array([label_to_id[l] for l in true_labels])\n",
    "\n",
    "\n",
    "# ===================== 2. Train Word2Vec =====================\n",
    "\n",
    "tokenized_docs = [doc.split() for doc in docs]\n",
    "\n",
    "print(\"Training Word2Vec model...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    epochs=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "# ===================== 3. Document Embeddings =====================\n",
    "\n",
    "def get_document_vector(doc_tokens, model):\n",
    "    valid_words = [w for w in doc_tokens if w in model.wv.key_to_index]\n",
    "    if not valid_words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[w] for w in valid_words], axis=0)\n",
    "\n",
    "print(\"Generating document embeddings...\")\n",
    "X_w2v = np.array([get_document_vector(tokens, w2v_model) for tokens in tokenized_docs])\n",
    "\n",
    "\n",
    "# ===================== 4. K-means =====================\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "y_pred_clusters = kmeans.fit_predict(X_w2v)\n",
    "\n",
    "\n",
    "# ===================== 5. Hungarian Label Matching =====================\n",
    "\n",
    "def match_labels(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    row_ind, col_ind = linear_sum_assignment(cm, maximize=True)\n",
    "    mapping = {col: row for col, row in zip(col_ind, row_ind)}\n",
    "    y_pred_matched = np.array([mapping[l] for l in y_pred])\n",
    "    return y_pred_matched, mapping\n",
    "\n",
    "y_pred_matched, mapping = match_labels(y_true, y_pred_clusters)\n",
    "\n",
    "\n",
    "# ===================== 6. Evaluation =====================\n",
    "\n",
    "sil_score = silhouette_score(X_w2v, y_pred_clusters)\n",
    "kappa = cohen_kappa_score(y_true, y_pred_matched)\n",
    "\n",
    "print(\"\\n=== K-means + Word2Vec Evaluation ===\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Kappa Score: {kappa:.4f}\")\n",
    "\n",
    "\n",
    "# ===================== 7. NPMI Coherence =====================\n",
    "\n",
    "cluster_top_words = []\n",
    "for i in range(k):\n",
    "    centroid = kmeans.cluster_centers_[i]\n",
    "    top_words = [w for w, _ in w2v_model.wv.similar_by_vector(centroid, topn=10)]\n",
    "    cluster_top_words.append(top_words)\n",
    "\n",
    "cv_vectorizer = CountVectorizer(\n",
    "    vocabulary=set(w for words in cluster_top_words for w in words),\n",
    "    binary=True\n",
    ")\n",
    "X_binary = cv_vectorizer.fit_transform(docs).toarray()\n",
    "word2id = cv_vectorizer.vocabulary_\n",
    "N = len(docs)\n",
    "\n",
    "coherence_scores = []\n",
    "for top_words in cluster_top_words:\n",
    "    score = 0\n",
    "    pairs = list(itertools.combinations(top_words, 2))\n",
    "    for w1, w2 in pairs:\n",
    "        if w1 not in word2id or w2 not in word2id:\n",
    "            continue\n",
    "        id1, id2 = word2id[w1], word2id[w2]\n",
    "        p_w1 = X_binary[:, id1].sum() / N\n",
    "        p_w2 = X_binary[:, id2].sum() / N\n",
    "        p_w12 = (X_binary[:, id1] * X_binary[:, id2]).sum() / N\n",
    "        if p_w12 > 0:\n",
    "            pmi = math.log(p_w12 / (p_w1 * p_w2))\n",
    "            npmi = pmi / -math.log(p_w12)\n",
    "            score += npmi\n",
    "    coherence_scores.append(score / len(pairs) if pairs else 0)\n",
    "\n",
    "avg_coherence = np.mean(coherence_scores)\n",
    "print(f\"NPMI Coherence: {avg_coherence:.4f}\")\n",
    "\n",
    "\n",
    "# ===================== 8. Confusion Matrix =====================\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_matched)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=unique_labels,\n",
    "            yticklabels=unique_labels)\n",
    "plt.title(\"Confusion Matrix (K-means + Word2Vec)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_kmeans_word2vec.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===================== 9. Error Analysis =====================\n",
    "\n",
    "print(\"\\n=== Error Analysis ===\")\n",
    "\n",
    "category_agreement = {}\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    correct = cm[i, i]\n",
    "    total = cm[i].sum()\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    category_agreement[label] = acc\n",
    "\n",
    "for k_label, v in category_agreement.items():\n",
    "    print(f\"{k_label}: {v:.3f}\")\n",
    "\n",
    "worst_category = min(category_agreement, key=category_agreement.get)\n",
    "print(\"\\nWorst clustered category:\", worst_category)\n",
    "\n",
    "worst_idx = label_to_id[worst_category]\n",
    "\n",
    "misclassified_docs = [\n",
    "    docs[i]\n",
    "    for i in range(len(docs))\n",
    "    if y_true[i] == worst_idx and y_pred_matched[i] != worst_idx\n",
    "]\n",
    "\n",
    "print(\"Misclassified samples:\", len(misclassified_docs))\n",
    "\n",
    "if len(misclassified_docs) > 0:\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=15)\n",
    "    X_err = vectorizer.fit_transform(misclassified_docs)\n",
    "    top_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(\"Top confusing words:\")\n",
    "    print(top_words)\n",
    "\n",
    "\n",
    "# ===================== 10. t-SNE Visualization =====================\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_w2v)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(\n",
    "    x=X_tsne[:,0],\n",
    "    y=X_tsne[:,1],\n",
    "    hue=[unique_labels[i] for i in y_true],\n",
    "    palette='Set1',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title(\"t-SNE (Word2Vec + K-means)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tsne_kmeans_word2vec.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n===== Pipeline Finished =====\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
