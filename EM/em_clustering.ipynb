{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a52290",
   "metadata": {},
   "source": [
    "# EM (GMM) Text Clustering Using Word2Vec (Full Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ccfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382d319",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f98b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../processed_data.csv\")\n",
    "\n",
    "texts = data[\"Cleaned_Content\"].astype(str)\n",
    "true_labels = data[\"Label\"]\n",
    "\n",
    "tokenized_texts = [doc.split() for doc in texts]\n",
    "\n",
    "print(\"Dataset size:\", len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f1da5",
   "metadata": {},
   "source": [
    "## Word2Vec Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Word2Vec...\")\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "def document_vector(tokens):\n",
    "    vectors = [\n",
    "        w2v_model.wv[word]\n",
    "        for word in tokens\n",
    "        if word in w2v_model.wv\n",
    "    ]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X = np.array([document_vector(doc) for doc in tokenized_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe712184",
   "metadata": {},
   "source": [
    "## EM / GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(np.unique(true_labels))\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=n_clusters,\n",
    "    covariance_type='full',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pred_clusters = gmm.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8124c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil = silhouette_score(X, pred_clusters)\n",
    "print(\"Silhouette:\", sil)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "true_encoded = encoder.fit_transform(true_labels)\n",
    "\n",
    "kappa = cohen_kappa_score(true_encoded, pred_clusters)\n",
    "print(\"Kappa:\", kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d8f79",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ee697",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_encoded, pred_clusters)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Word2Vec GMM\")\n",
    "plt.xlabel(\"Predicted Cluster\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c4bcf",
   "metadata": {},
   "source": [
    "## Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b660cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "cluster_topics = []\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    cluster_docs = texts[pred_clusters == c]\n",
    "    if len(cluster_docs) > 0:\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            max_features=10\n",
    "        )\n",
    "        X_counts = vectorizer.fit_transform(cluster_docs)\n",
    "        words = vectorizer.get_feature_names_out()\n",
    "        cluster_topics.append(list(words))\n",
    "\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=cluster_topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "coherence = coherence_model.get_coherence()\n",
    "print(\"Coherence:\", coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201984c5",
   "metadata": {},
   "source": [
    "## PCA for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(\n",
    "    X_2d[:,0],\n",
    "    X_2d[:,1],\n",
    "    c=pred_clusters,\n",
    "    cmap=\"viridis\",\n",
    "    s=25\n",
    ")\n",
    "\n",
    "plt.title(\"GMM Clustering (Word2Vec)\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "def draw_ellipse(position, covariance):\n",
    "    if covariance.shape == (2,2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1,0], U[0,0]))\n",
    "        width, height = 2*np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2*np.sqrt(covariance)\n",
    "    for nsig in range(1,4):\n",
    "        plt.gca().add_patch(\n",
    "            Ellipse(\n",
    "                position,\n",
    "                nsig*width,\n",
    "                nsig*height,\n",
    "                angle=angle,\n",
    "                fill=False,\n",
    "                linewidth=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "for i in range(gmm.n_components):\n",
    "    mean_2d = pca.transform(\n",
    "        gmm.means_[i].reshape(1,-1)\n",
    "    )[0]\n",
    "    cov_2d = (\n",
    "        pca.components_\n",
    "        @ gmm.covariances_[i]\n",
    "        @ pca.components_.T\n",
    "    )\n",
    "    draw_ellipse(mean_2d, cov_2d)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7b8e1",
   "metadata": {},
   "source": [
    "## Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(X[:200], method='ward')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "dendrogram(linked, truncate_mode=\"level\", p=5)\n",
    "plt.title(\"Hierarchical Dendrogram (Word2Vec)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573f213",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f694a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Predicted\"] = pred_clusters\n",
    "data[\"True\"] = true_encoded\n",
    "\n",
    "errors = data[data[\"Predicted\"] != data[\"True\"]]\n",
    "\n",
    "print(\"Misclustered Samples:\", len(errors))\n",
    "\n",
    "if len(errors) > 0:\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        max_features=10\n",
    "    )\n",
    "    X_err = vectorizer.fit_transform(\n",
    "        errors[\"Cleaned_Content\"]\n",
    "    )\n",
    "    print(\"Top confusing words:\")\n",
    "    print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\n===== WORD2VEC PIPELINE FINISHED =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339e3ec",
   "metadata": {},
   "source": [
    "## Error Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(model_name, data, true_encoded, pred_clusters, texts, encoder):\n",
    "    print(f\"\\n===== ERROR ANALYSIS: {model_name} =====\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_encoded, pred_clusters)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted Cluster\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Category agreement\n",
    "    label_names = encoder.classes_\n",
    "    category_agreement = {}\n",
    "\n",
    "    for i, label in enumerate(label_names):\n",
    "        correct = cm[i, i]\n",
    "        total = cm[i].sum()\n",
    "        acc = correct / total if total > 0 else 0\n",
    "        category_agreement[label] = acc\n",
    "\n",
    "    print(\"\\nCategory-wise agreement:\")\n",
    "    for k, v in category_agreement.items():\n",
    "        print(k, round(v, 3))\n",
    "\n",
    "    # Worst category\n",
    "    worst_category = min(category_agreement, key=category_agreement.get)\n",
    "    print(\"Worst clustered category:\", worst_category)\n",
    "\n",
    "    worst_index = list(label_names).index(worst_category)\n",
    "\n",
    "    misclassified = data[\n",
    "        (true_encoded == worst_index) &\n",
    "        (pred_clusters != worst_index)\n",
    "    ]\n",
    "\n",
    "    print(\"Misclassified samples:\", len(misclassified))\n",
    "\n",
    "    # Top confusing words\n",
    "    if len(misclassified) > 0:\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            max_features=15\n",
    "        )\n",
    "        X_err = vectorizer.fit_transform(misclassified[\"Cleaned_Content\"])\n",
    "        top_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "        print(\"Top confusing words:\")\n",
    "        print(top_words)\n",
    "\n",
    "        # Save to txt file\n",
    "        with open(f\"Error_{model_name}_TopWords.txt\", \"w\") as f:\n",
    "            f.write(\"Worst Category: \" + worst_category + \"\\n\")\n",
    "            f.write(\"Misclassified Samples: \" + str(len(misclassified)) + \"\\n\\n\")\n",
    "            f.write(\"Top Confusing Words:\\n\")\n",
    "            for w in top_words:\n",
    "                f.write(w + \"\\n\")\n",
    "\n",
    "error_analysis(\"Word2Vec_GMM\", data, true_encoded, pred_clusters, texts, encoder)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
