{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4444ebd3",
   "metadata": {},
   "source": [
    "1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf23b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Configuration / Settings\n",
    "DATASETS_FOLDER = 'datasets'\n",
    "OUTPUT_FILE = 'processed_data.csv'\n",
    "CHUNK_SIZE = 150\n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "RANDOM_SEED = 51\n",
    "\n",
    "# Get NLTK English stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LABEL_RELATED_WORDS = {\n",
    "    'electronic health records', 'health records', 'EHR', 'ehr',\n",
    "    'healthcare robotics', \n",
    "    'medical imaging',  \n",
    "    'precision medicine', \n",
    "    'telemedicine', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aae1da",
   "metadata": {},
   "source": [
    "2. Process Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698146b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 CSV files in 'datasets':\n",
      "\n",
      "--- Processing: electronic health records.csv ---\n",
      "  Sampling: Selected 200 out of 20000 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "--- Processing: healthcare robotics.csv ---\n",
      "  Sampling: Selected 200 out of 7861 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "--- Processing: medical imaging.csv ---\n",
      "  Sampling: Selected 200 out of 20000 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "--- Processing: precision medicine.csv ---\n",
      "  Sampling: Selected 200 out of 20000 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "--- Processing: telemedicine.csv ---\n",
      "  Sampling: Selected 200 out of 20000 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "Processing complete. Total records collected: 1000\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to store results\n",
    "results = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(DATASETS_FOLDER):\n",
    "    print(f\"Error: Folder '{DATASETS_FOLDER}' not found.\")\n",
    "else:\n",
    "    # Get list of CSV files\n",
    "    files = [f for f in os.listdir(DATASETS_FOLDER) if f.endswith('.csv')]\n",
    "    print(f\"Found {len(files)} CSV files in '{DATASETS_FOLDER}':\\n\")\n",
    "\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(DATASETS_FOLDER, filename)\n",
    "        label = os.path.splitext(filename)[0]\n",
    "        \n",
    "        print(f\"--- Processing: {filename} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check for required columns\n",
    "            if 'Abstract' not in df.columns or 'Title' not in df.columns:\n",
    "                print(f\"  [Skipped] Required columns 'Abstract' or 'Title' missing.\")\n",
    "                continue\n",
    "            \n",
    "            # Remove rows where Abstract is NA or '[No abstract available]'\n",
    "            initial_count = len(df)\n",
    "            df = df.dropna(subset=['Abstract'])\n",
    "         \n",
    "            df = df[df['Abstract'].str.strip().str.lower() != '[no abstract available]']\n",
    "            \n",
    "            filtered_count = len(df)\n",
    "            if filtered_count < initial_count:\n",
    "                print(f\"  Filtered out {initial_count - filtered_count} records with missing or '[No abstract available]' content.\")\n",
    "\n",
    "            total_docs = len(df)\n",
    "            \n",
    "            # Randomly sample 200 documents\n",
    "            if total_docs > SAMPLE_SIZE:\n",
    "                sampled_df = df.sample(n=SAMPLE_SIZE, random_state=RANDOM_SEED)\n",
    "                print(f\"  Sampling: Selected {SAMPLE_SIZE} out of {total_docs} documents.\")\n",
    "            else:\n",
    "                sampled_df = df\n",
    "                print(f\"  Taking all: Used all {total_docs} documents.\")\n",
    "            \n",
    "            # Process Abstract content\n",
    "            file_record_count = 0\n",
    "            for _, row in sampled_df.iterrows():\n",
    "                abstract = row['Abstract']\n",
    "                paper_name = row['Title']\n",
    "                \n",
    "                # Fetch additional fields\n",
    "                # Using .get() to avoid errors if columns are missing in some files\n",
    "                doc_type = row.get('Document Type', '')\n",
    "                affiliations = row.get('Affiliations', '')\n",
    "\n",
    "                # Handle missing or non-string abstracts (Double check)\n",
    "                if pd.isna(abstract) or not isinstance(abstract, str):\n",
    "                    continue\n",
    "                \n",
    "                # Split into words\n",
    "                words = abstract.split()\n",
    "                \n",
    "                # 1. Take first 150 words.\n",
    "                # 2. Truncate if longer.\n",
    "                # 3. Pad if shorter.\n",
    "                \n",
    "                if len(words) >= CHUNK_SIZE:\n",
    "                    chunk = words[:CHUNK_SIZE]\n",
    "                else:\n",
    "                    # Pad with a placeholder token\n",
    "                    chunk = words + ['[PAD]'] * (CHUNK_SIZE - len(words))\n",
    "                \n",
    "                chunk_text = ' '.join(chunk)\n",
    "                \n",
    "                results.append({\n",
    "                    'Content': chunk_text,\n",
    "                    'Paper Name': paper_name,\n",
    "                    'Label': label,\n",
    "                    'Document Type': doc_type,\n",
    "                    'Affiliations': affiliations\n",
    "                })\n",
    "                file_record_count += 1\n",
    "            \n",
    "            print(f\"  > Generated {file_record_count} valid records from this file.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [Error] Failed to read {filename}: {e}\\n\")\n",
    "\n",
    "print(f\"Processing complete. Total records collected: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71182491",
   "metadata": {},
   "source": [
    "3. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44b8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to clean text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove URLs, emails, and HTML tags if any\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 3. Remove special characters and numbers (keep letters and spaces)\n",
    "    # The regex [^a-zA-Z\\s] means replace all characters except letters and whitespace with a space\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # 4. Tokenize and remove stopwords, junk characters, lemmatize\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove words that are too short (e.g. single letters, except 'a', 'i', etc., usually meaningless, filtered here)\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "            \n",
    "        lemma_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "        # Prioritize checking if masking is needed (Before stopword check to ensure we mask even if it somehow was a stopword, though unlikely)\n",
    "        if word in LABEL_RELATED_WORDS or lemma_word in LABEL_RELATED_WORDS:\n",
    "            cleaned_words.append('[Cluster]')\n",
    "            continue\n",
    "\n",
    "        # Check if in stop words list (using base stop_words)\n",
    "        if word not in STOP_WORDS and lemma_word not in STOP_WORDS:\n",
    "            cleaned_words.append(lemma_word)\n",
    "            \n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "if results:\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    result_df[\"Cleaned_Content\"] = result_df[\"Content\"].apply(clean_text)\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"[Warning] No data generated. Review the source files and logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7eaf9",
   "metadata": {},
   "source": [
    "4. Save the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046cb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 1000\n",
      "Total Columns: 6\n",
      "\n",
      "--- Class Distribution (Records per Label) ---\n",
      "Label\n",
      "electronic health records    200\n",
      "healthcare robotics          200\n",
      "medical imaging              200\n",
      "precision medicine           200\n",
      "telemedicine                 200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- First 5 Records ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Paper Name</th>\n",
       "      <th>Label</th>\n",
       "      <th>Document Type</th>\n",
       "      <th>Affiliations</th>\n",
       "      <th>Cleaned_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objective: This study aimed to explore the cha...</td>\n",
       "      <td>Exploring the Challenges Student Pharmacists C...</td>\n",
       "      <td>electronic health records</td>\n",
       "      <td>Article</td>\n",
       "      <td></td>\n",
       "      <td>objective study aimed explore challenge pharma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Introduction: Breast cancer accounted for 21.9...</td>\n",
       "      <td>Ten-year survival in early-stage breast cancer...</td>\n",
       "      <td>electronic health records</td>\n",
       "      <td>Article</td>\n",
       "      <td></td>\n",
       "      <td>introduction breast cancer accounted cancer de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction: There has been a steady decline ...</td>\n",
       "      <td>Association Between Opioid Dosage Tapering and...</td>\n",
       "      <td>electronic health records</td>\n",
       "      <td>Article</td>\n",
       "      <td></td>\n",
       "      <td>introduction steady decline national opioid di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Objective: Social determinants of health (SDoH...</td>\n",
       "      <td>Screening for Social Determinants of Health in...</td>\n",
       "      <td>electronic health records</td>\n",
       "      <td>Article</td>\n",
       "      <td></td>\n",
       "      <td>objective social determinant health sdoh impac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background: The Medical Informatics Initiative...</td>\n",
       "      <td>Leveraging Interoperable Electronic Health Rec...</td>\n",
       "      <td>electronic health records</td>\n",
       "      <td>Article</td>\n",
       "      <td></td>\n",
       "      <td>background medical informatics initiative mii ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  Objective: This study aimed to explore the cha...   \n",
       "1  Introduction: Breast cancer accounted for 21.9...   \n",
       "2  Introduction: There has been a steady decline ...   \n",
       "3  Objective: Social determinants of health (SDoH...   \n",
       "4  Background: The Medical Informatics Initiative...   \n",
       "\n",
       "                                          Paper Name  \\\n",
       "0  Exploring the Challenges Student Pharmacists C...   \n",
       "1  Ten-year survival in early-stage breast cancer...   \n",
       "2  Association Between Opioid Dosage Tapering and...   \n",
       "3  Screening for Social Determinants of Health in...   \n",
       "4  Leveraging Interoperable Electronic Health Rec...   \n",
       "\n",
       "                       Label Document Type Affiliations  \\\n",
       "0  electronic health records       Article                \n",
       "1  electronic health records       Article                \n",
       "2  electronic health records       Article                \n",
       "3  electronic health records       Article                \n",
       "4  electronic health records       Article                \n",
       "\n",
       "                                     Cleaned_Content  \n",
       "0  objective study aimed explore challenge pharma...  \n",
       "1  introduction breast cancer accounted cancer de...  \n",
       "2  introduction steady decline national opioid di...  \n",
       "3  objective social determinant health sdoh impac...  \n",
       "4  background medical informatics initiative mii ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Success] Data saved to: processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "# --- Display Info ---\n",
    "    print(f\"Total Rows: {result_df.shape[0]}\")\n",
    "    print(f\"Total Columns: {result_df.shape[1]}\")\n",
    "    \n",
    "    print(\"\\n--- Class Distribution (Records per Label) ---\")\n",
    "    print(result_df['Label'].value_counts())\n",
    "    \n",
    "    print(\"\\n--- First 5 Records ---\")\n",
    "    try:\n",
    "        display(result_df.head())\n",
    "    except NameError:\n",
    "        print(result_df.head())\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n[Success] Data saved to: {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
