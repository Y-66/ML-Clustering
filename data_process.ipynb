{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4444ebd3",
   "metadata": {},
   "source": [
    "1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "1bf23b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T18:42:58.982510Z",
     "start_time": "2026-02-25T18:42:58.942850Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Configuration / Settings\n",
    "DATASETS_FOLDER = 'datasets'\n",
    "OUTPUT_FILE = 'processed_data.csv'\n",
    "CHUNK_SIZE = 150\n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "RANDOM_SEED = 51\n",
    "\n",
    "# Get NLTK English stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LABEL_RELATED_WORDS = {\n",
    "    'electronic health records', 'health records', 'EHR', 'ehr',\n",
    "    'healthcare robotics', \n",
    "    'medical imaging',  \n",
    "    'precision medicine', \n",
    "    'telemedicine', \n",
    "}"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mrandom\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mre\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnltk\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcorpus\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m stopwords\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnltk\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mstem\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m WordNetLemmatizer\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Configuration / Settings\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'nltk'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "e1aae1da",
   "metadata": {},
   "source": [
    "2. Process Data Files"
   ]
  },
  {
   "cell_type": "code",
   "id": "698146b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T18:43:02.559324Z",
     "start_time": "2026-02-25T18:43:02.482561Z"
    }
   },
   "source": [
    "# Initialize list to store results\n",
    "results = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(DATASETS_FOLDER):\n",
    "    print(f\"Error: Folder '{DATASETS_FOLDER}' not found.\")\n",
    "else:\n",
    "    # Get list of CSV files\n",
    "    files = [f for f in os.listdir(DATASETS_FOLDER) if f.endswith('.csv')]\n",
    "    print(f\"Found {len(files)} CSV files in '{DATASETS_FOLDER}':\\n\")\n",
    "\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(DATASETS_FOLDER, filename)\n",
    "        label = os.path.splitext(filename)[0]\n",
    "        \n",
    "        print(f\"--- Processing: {filename} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check for required columns\n",
    "            if 'Abstract' not in df.columns or 'Title' not in df.columns:\n",
    "                print(f\"  [Skipped] Required columns 'Abstract' or 'Title' missing.\")\n",
    "                continue\n",
    "            \n",
    "            # Remove rows where Abstract is NA or '[No abstract available]'\n",
    "            initial_count = len(df)\n",
    "            df = df.dropna(subset=['Abstract'])\n",
    "         \n",
    "            df = df[df['Abstract'].str.strip().str.lower() != '[no abstract available]']\n",
    "            \n",
    "            filtered_count = len(df)\n",
    "            if filtered_count < initial_count:\n",
    "                print(f\"  Filtered out {initial_count - filtered_count} records with missing or '[No abstract available]' content.\")\n",
    "\n",
    "            total_docs = len(df)\n",
    "            \n",
    "            # Randomly sample 200 documents\n",
    "            if total_docs > SAMPLE_SIZE:\n",
    "                sampled_df = df.sample(n=SAMPLE_SIZE, random_state=RANDOM_SEED)\n",
    "                print(f\"  Sampling: Selected {SAMPLE_SIZE} out of {total_docs} documents.\")\n",
    "            else:\n",
    "                sampled_df = df\n",
    "                print(f\"  Taking all: Used all {total_docs} documents.\")\n",
    "            \n",
    "            # Process Abstract content\n",
    "            file_record_count = 0\n",
    "            for _, row in sampled_df.iterrows():\n",
    "                abstract = row['Abstract']\n",
    "                paper_name = row['Title']\n",
    "                \n",
    "                # Fetch additional fields\n",
    "                # Using .get() to avoid errors if columns are missing in some files\n",
    "                doc_type = row.get('Document Type', '')\n",
    "                affiliations = row.get('Affiliations', '')\n",
    "\n",
    "                # Handle missing or non-string abstracts (Double check)\n",
    "                if pd.isna(abstract) or not isinstance(abstract, str):\n",
    "                    continue\n",
    "                \n",
    "                # Split into words\n",
    "                words = abstract.split()\n",
    "                \n",
    "                # 1. Take first 150 words.\n",
    "                # 2. Truncate if longer.\n",
    "                # 3. Pad if shorter.\n",
    "                \n",
    "                if len(words) >= CHUNK_SIZE:\n",
    "                    chunk = words[:CHUNK_SIZE]\n",
    "                else:\n",
    "                    # Pad with a placeholder token\n",
    "                    chunk = words + ['[PAD]'] * (CHUNK_SIZE - len(words))\n",
    "                \n",
    "                chunk_text = ' '.join(chunk)\n",
    "                \n",
    "                results.append({\n",
    "                    'Content': chunk_text,\n",
    "                    'Paper Name': paper_name,\n",
    "                    'Label': label,\n",
    "                    'Document Type': doc_type,\n",
    "                    'Affiliations': affiliations\n",
    "                })\n",
    "                file_record_count += 1\n",
    "            \n",
    "            print(f\"  > Generated {file_record_count} valid records from this file.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [Error] Failed to read {filename}: {e}\\n\")\n",
    "\n",
    "print(f\"Processing complete. Total records collected: {len(results)}\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Initialize list to store results\u001B[39;00m\n\u001B[32m      2\u001B[39m results = []\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m lemmatizer = \u001B[43mWordNetLemmatizer\u001B[49m()\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Check if folder exists\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.exists(DATASETS_FOLDER):\n",
      "\u001B[31mNameError\u001B[39m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "71182491",
   "metadata": {},
   "source": [
    "3. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "a44b8aa3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-25T18:42:16.617305Z"
    }
   },
   "source": [
    "# define a function to clean text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove URLs, emails, and HTML tags if any\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 3. Remove special characters and numbers (keep letters and spaces)\n",
    "    # The regex [^a-zA-Z\\s] means replace all characters except letters and whitespace with a space\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # 4. Tokenize and remove stopwords, junk characters, lemmatize\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove words that are too short (e.g. single letters, except 'a', 'i', etc., usually meaningless, filtered here)\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "            \n",
    "        lemma_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "        # # Prioritize checking if masking is needed (Before stopword check to ensure we mask even if it somehow was a stopword, though unlikely)\n",
    "        # if word in LABEL_RELATED_WORDS or lemma_word in LABEL_RELATED_WORDS:\n",
    "        #     cleaned_words.append('[Cluster]')\n",
    "        #     continue\n",
    "\n",
    "        # Check if in stop words list (using base stop_words)\n",
    "        if word not in STOP_WORDS and lemma_word not in STOP_WORDS:\n",
    "            cleaned_words.append(lemma_word)\n",
    "            \n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "if results:\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    result_df[\"Cleaned_Content\"] = result_df[\"Content\"].apply(clean_text)\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"[Warning] No data generated. Review the source files and logic.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60c7eaf9",
   "metadata": {},
   "source": [
    "4. Save the File"
   ]
  },
  {
   "cell_type": "code",
   "id": "046cb84b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T18:42:16.617305Z",
     "start_time": "2026-02-25T18:42:16.617305Z"
    }
   },
   "source": [
    "if results:\n",
    "# --- Display Info ---\n",
    "    print(f\"Total Rows: {result_df.shape[0]}\")\n",
    "    print(f\"Total Columns: {result_df.shape[1]}\")\n",
    "    \n",
    "    print(\"\\n--- Class Distribution (Records per Label) ---\")\n",
    "    print(result_df['Label'].value_counts())\n",
    "    \n",
    "    print(\"\\n--- First 5 Records ---\")\n",
    "    try:\n",
    "        display(result_df.head())\n",
    "    except NameError:\n",
    "        print(result_df.head())\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n[Success] Data saved to: {OUTPUT_FILE}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
